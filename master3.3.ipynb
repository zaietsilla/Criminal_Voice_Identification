{"cells":[{"cell_type":"code","execution_count":null,"id":"01bebf9b-2c48-475a-9028-fccf8fc0bdd9","metadata":{"id":"01bebf9b-2c48-475a-9028-fccf8fc0bdd9","outputId":"7dac8d48-c08f-4208-9f05-6d7eb1dee293"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -q pyannote.audio"]},{"cell_type":"code","execution_count":null,"id":"276c6968-308b-4561-a4fe-0e1c963c7187","metadata":{"id":"276c6968-308b-4561-a4fe-0e1c963c7187"},"outputs":[],"source":["import os\n","import torch\n","import torchaudio\n","import numpy as np\n","\n","from sklearn.cluster import KMeans\n","from pyannote.audio import Pipeline, Model, Inference\n","from scipy.spatial.distance import cosine, cdist, euclidean"]},{"cell_type":"code","execution_count":null,"id":"b22c0771-23f4-49c2-9d20-759e3bfcb35d","metadata":{"id":"b22c0771-23f4-49c2-9d20-759e3bfcb35d","outputId":"2099a6f1-56b7-435c-b146-f1ee61f17693"},"outputs":[{"name":"stderr","output_type":"stream","text":["Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--embedding/snapshots/c6335d8f1cd77b30084387468a6cf26fea90009b/pytorch_model.bin`\n","Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--embedding/snapshots/c6335d8f1cd77b30084387468a6cf26fea90009b/pytorch_model.bin`\n"]},{"name":"stdout","output_type":"stream","text":["Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n","Model was trained with torch 1.8.1+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n","Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n","Model was trained with torch 1.8.1+cu102, yours is 2.1.0+cu118. Bad things might happen unless you revert torch to 1.x.\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– vik_roz_720_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– vik_roz_540_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– vik_roz_360_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– vik_roz_180_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– vik_roz_0_150.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– yarema_dukh_490_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– yarema_dukh_900_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– yarema_dukh_720_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– yarema_dukh_1080_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– yarema_dukh_0_180.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– tetiana_mykytenko_0_190.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– tetiana_mykytenko_470_190.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– tetiana_mykytenko_203_198.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– test_840_180_n.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– tetiana_mykytenko_4332_178.wav\n","Ð—Ð»Ð¾Ð²Ð¼Ð¸ÑÐ½Ð¸Ðº Ð²Ð¸ÑÐ²Ð»ÐµÐ½Ð¸Ð¹ Ñƒ Ñ„Ð°Ð¹Ð»Ñ– tetiana_mykytenko_3960_219.wav\n","CPU times: user 36min 39s, sys: 7.03 s, total: 36min 46s\n","Wall time: 6min 22s\n"]}],"source":["%%time\n","\n","# Determining the device (CUDA or CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Hugging Face User Access Tokens\n","AUTH_TOKEN = \"ðŸ”‘ðŸŒŸ AUTH_TOKEN ðŸŒŸðŸ”‘\"\n","\n","# Initialization of the diarization pipeline and model for obtaining embeddings\n","diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=AUTH_TOKEN).to(device)\n","\n","model = Model.from_pretrained(\"pyannote/embedding\", use_auth_token=AUTH_TOKEN).to(device)\n","embedding_inference = Inference(model)\n","\n","\n","# Function for computing the embedding from an audio file\n","def compute_embedding(audio_path):\n","    embeddings = embedding_inference(audio_path)\n","    if len(embeddings) > 0:\n","        return np.mean(embeddings, axis=0)\n","    else:\n","        return np.zeros(embedding_inference.model.dimension)\n","\n","\n","min_segment_duration = 5  # Minimum segment duration in seconds\n","\n","\n","# Function for extracting speaker embeddings from a podcast\n","def get_speaker_embeddings(podcast_path):\n","    diarization = diarization_pipeline(podcast_path)\n","    embeddings = []\n","    segments = []\n","    waveform, sample_rate = torchaudio.load(podcast_path)\n","    waveform = waveform.to(device)\n","\n","    for segment, _, label in diarization.itertracks(yield_label=True):\n","        if segment.duration < min_segment_duration:\n","            continue\n","        start_time = segment.start\n","        end_time = segment.end\n","        segment_embedding = compute_embedding_for_segment(podcast_path, start_time, end_time, sample_rate)\n","        embeddings.append(segment_embedding)\n","        segments.append(segment)\n","    return embeddings, segments\n","\n","\n","# Function for computing the embedding for a specific segment\n","def compute_embedding_for_segment(audio_path, start_time, end_time, sample_rate):\n","    waveform, _ = torchaudio.load(audio_path, frame_offset=int(start_time * sample_rate), num_frames=int((end_time - start_time) * sample_rate))\n","    waveform = waveform.to(device)\n","    embeddings = embedding_inference({'waveform': waveform, 'sample_rate': sample_rate})\n","    if len(embeddings) > 0:\n","        return np.mean(embeddings, axis=0)\n","    else:\n","        return np.zeros(embedding_inference.model.dimension)\n","\n","\n","# Function for extracting embeddings of all audio files in a folder\n","def get_embeddings_from_folder(folder_path):\n","    embeddings = []\n","    for file_name in os.listdir(folder_path):\n","        if file_name.lower().endswith(('.wav')):\n","            file_path = os.path.join(folder_path, file_name)\n","            embeddings.append(compute_embedding(file_path))\n","    return embeddings\n","\n","\n","# Extracting criminal embeddings\n","criminal_folder_path = \"criminals\"\n","criminal_embeddings = get_embeddings_from_folder(criminal_folder_path)\n","\n","\n","# Clustering of criminal embeddings\n","def cluster_criminals_embeddings(criminal_embeddings, n_clusters=3):\n","    kmeans = KMeans(n_clusters=n_clusters)\n","    kmeans.fit(criminal_embeddings)\n","    return kmeans.cluster_centers_\n","\n","\n","# Threshold of similarity for the identification of criminals\n","similarity_threshold = 0.4\n","\n","\n","# Comparison of the segment with clusters of criminals.\n","def is_criminal_segment(speaker_embedding, criminal_clusters, threshold=similarity_threshold):\n","    distances = cdist([speaker_embedding], criminal_clusters, metric=\"cosine\")\n","    return any(distance < threshold for distance in distances[0])\n","\n","\n","criminal_clusters = cluster_criminals_embeddings(criminal_embeddings)\n","\n","\n","# Path to the podcast folder\n","podcast_folder_path = \"podcasts\"\n","\n","\n","# Processing of each podcast in the folder\n","for file_name in os.listdir(podcast_folder_path):\n","    if file_name.lower().endswith(('.wav')):\n","        podcast_path = os.path.join(podcast_folder_path, file_name)\n","        speaker_embeddings, segments = get_speaker_embeddings(podcast_path)\n","        criminal_found = False\n","\n","        for speaker_embedding, segment in zip(speaker_embeddings, segments):\n","            if is_criminal_segment(speaker_embedding, criminal_clusters):\n","                criminal_found = True\n","                break  # Stop further scanning of this file\n","\n","        if criminal_found:\n","            print(f\"Criminal detected in the file {file_name}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}